# 回归分析
## 1.1是什么
回归分析是一种统计分析方式，用于预测一个连续变量与一个或是多个其他变量之间的关系。回归分析通过建立模型来预测因变量（被预测变量）与自变量（预测变量）之间的关系，从而预测被预测变量的值
## 1.2多种形式
线性回归、非线性回归、logistics回归等等
## 1.3几种的区别
线性回归：假设因变量与自变量之间存在线性关系，即因变量与自变量的关系可以用直线来描述，其通常用于预测数值型变量，如根据房屋面积、房龄等因素预测房价等

非线性回归：假设因变量和自变量之间不存在线性关系，即因变量与自变量的关系需要用更为复杂的函数模型来描述，例如预测某商家的销售额——我们通过分析广告费用、宣传渠道等预测销售额的变化趋势。

logistic回归：常常用于二元分类问题，例如预测某个用户是否会购买某种商品或预测某个患者是否患有某种疾病。该模型往往建立在Sigmoid函数的基础上。该函数可以将任意实数映射到0-1的范围内，表示某事件的概率
# 2.线性回归
## 2.1线性
线性包括了可加性和齐次性
### 2.1.1可加性
也称作是叠加性，即当函数$f$满足下式

$$
f(x+y)=f(x)+f(y)
$$

则我们称函数f具有可加性
### 2.1.2齐次性
也称作是均匀性，即当函数$f$满足下式

$$
f(ax)=af(x)
$$

其中a为与x无关的常数，则我们称函数$f$具有齐次性

当函数$f$同时具有可加性与齐次性时，即函数$f$满足

$$f(ax+by)=af(x)+bf(y)
$$

我们称函数$f$为线性函数
## 2.2回归
解决回归问题，实际上就是找到一条线/超平面来拟合这些样本点，使他们之间的误差尽可能的小。我们所构建的不同的线/平面在同一个数据集下对应着不同的误差，我们需要找到使得误差最小的线/超平面，这就是回归的目的。
### 2.2.1损失函数
如何定量描述我们上面说到的这种误差来确定我们的拟合函数呢？

我们引入损失函数（误差函数）来衡量误差

常用的损失函数有：
* 均方误差MSE：$\frac{1}{m}\displaystyle\sum_{i=1}^m(y_i-\hat{y_i})^2$
* 均方根误差RMSE：$\sqrt{\frac{1}{m}\displaystyle\sum_{i=1}^m(y_i-\hat{y_i})^2}$
* 平均绝对误差MAE：$\frac{1}{m}\displaystyle\sum_{i=1}^m|y_i-\hat{y_i}|$
### 2.2.2回归任务的模型评估方式
R-squared:

$$
R^2(y,\hat{y})=1-\frac{\displaystyle\sum_{i=0}^n(y_i-\hat{y_i})^2}{\displaystyle\sum_{i=0}^n(y_i-\bar{y})^2}
$$

此方式我们可以理解为按照模型预测，比较模型预测和按照平均值预测的比例，此比例越小，那么模型就越精确，即上式越接近1，模型拟合的越好；当该式子出现负数的情况时，说明该模型的预测不如全部按照平均值进行预测（式子值=0）
### 2.2.3最小化损失函数
我们再一次强调：==回归任务的目的是拟合样本点，使得误差尽可能的小==
### 2.2.4最小二乘法
#### 2.2.4.1一元线性函数
我们先以一元线性函数为例考虑线性回归，之后再引入多元线性回归
考虑一元线性函数$y=kx+b$，并采用均方误差作为我们的损失函数，则损失函数为关于变量k和b的函数：

$$
L(k,b)=\frac{1}{m}\displaystyle\sum_{i=1}^m((kx_i+b)-y_i)^2
$$

其中，m为样本个数；而我们的回归任务就是找到这个$L(k,b)$函数的最小值。

我们通过求偏导数的方法来求这个函数的最小值。对L函数求b的偏导（过程比较简单不再列出），我们可以求出当偏导为0时有：

$$
k\displaystyle\sum_{i=1}^mx_i+\displaystyle\sum_{i=1}^mb-\displaystyle\sum_{i=1}^my_i=0
$$

即$km\bar{x}+mb-m\bar{y}=0$

即$b=\bar{y}-k\bar{x}$(联系到均值公式)

同样的，我们对L函数求k的偏导，（过程简单省去），则当偏导为0时我们有：

$$
k\displaystyle\sum_{i=1}^mx_i^2+b\displaystyle\sum_{i=1}^mx_i-\displaystyle\sum_{i=1}^mx_iy_i
$$

考虑x均值的情况下此式改写为：

$$
k\displaystyle\sum_{i=1}^mx_i^2+mb\bar{x}-\displaystyle\sum_{i=1}^mx_iy_i=0
$$

结合最小时$b=\bar{y}-k\bar{x}$,我们消去b可以得到：

$$
k(\displaystyle\sum_{i=1}^mx_i^2-m\bar{x}^2)=\displaystyle\sum_{i=1}^mx_iy_i-m\bar{y}\bar{x}
$$

从而我们可也解得

$$
k=\frac{\displaystyle\sum_{i=1}^mx_iy_i-m\bar{x}\bar{y}}{\displaystyle\sum_{i=1}^mx_i^2-m\bar{x}^2}
$$

这样我们就得到了使得该损失函数最小的k，b参数。
#### 2.2.4.2多元线性函数
现在我们将一元变量推广到多元变量，考虑多元函数式为

$$
f(x_1,x_2,\ldots,x_n)=\omega_1x_1+\omega_2x_2+\ldots+\omega_nx_n+b
$$

我们通过线性代数向量概念对该式子进行整理，不妨$\omega_0=b,x_0=1$，那么我们有权重向量$\omega$和特征向量$x$，且

$$
\omega=\begin{pmatrix}\omega_0,&\omega_1,&\omega_2,&\ldots,&\omega_n\end{pmatrix}
$$

$$
x=\begin{pmatrix}x_0,&x_1,&x_2,&\ldots,&x_n\end{pmatrix}
$$

则我们的多元函数可以写成$f(x)=\omega^Tx或f(x)=x^T\omega$
从而我们的损失函数可以写成如下形式:

$$
L(\omega)=\displaystyle\sum_{i=1}^m(y_i-\omega^Tx^{(i)})^2
$$

其中，$y_i$是第i个真实值,$x^{(i)}$第i个样本的特征向量（PS：这里要注意，真实值，标签值和样本值这三个词的意思其实是一样的）

继续，我们希望对式子进行进一步的简化，定义如下：
* 标签向量：将m个样本标签值堆叠成一个标签向量y，$y=(y_1,y_2,\ldots,y_m),
==在线性代数当中，见到一个向量均默认为列向量，转置的才为行向量==
* 样本矩阵X：定义样本矩阵X，形状为(m,n+1)的矩阵，其中有m个样本，n+1个特征（其中第1个特征向量的值均为1）

$$
\begin{pmatrix}1&x_1^{(1)}&x_2^{(1)}&\dots&x_n^{(1)}\\1&x_1^{(2)}&x_2^{(2)}&\dots&x_n^{(2)}\\\vdots&\vdots&\vdots&\ddots&\vdots\\1&x_1^{(m)}&x_2^{(m)}&\dots&x_n^{(m)}\end{pmatrix}
$$

在该矩阵当中，每一行是一个样本，每一列是一个特征(向量)
从而我们可以得到：将样本矩阵与权重向量相乘得到预测值向量$\hat{y}$

即：

$$
\begin{pmatrix}1&x_1^{(1)}&x_2^{(1)}&\dots&x_n^{(1)}\\1&x_1^{(2)}&x_2^{(2)}&\dots&x_n^{(2)}\\\vdots&\vdots&\vdots&\ddots&\vdots\\1&x_1^{(m)}&x_2^{(m)}&\dots&x_n^{(m)}\end{pmatrix}\begin{pmatrix}
\omega_0\\\vdots\\\omega_n    
\end{pmatrix}=\begin{pmatrix}
\hat{y}_0\\\vdots\\\hat{y}_m    
\end{pmatrix}
$$

##### 2.2.4.2.12范数
同时我们还需要借用2范数进行化简，下作介绍：

2范数：向量各个元素的平方和的平方根，即$||x||_2=\sqrt{\displaystyle\sum_{i=1}^nx_i^2}$

其中|| ||是范数符号，下标2表示为2范数，其有以下特性：
$x^Tx=||x||_2^2=\displaystyle\sum_{i=1}^nx_i^2$

又，此时我们的损失函数已经简化为如下形式：$$L(\omega)=||X\omega-y||^2$$

==注意矩阵和向量的形状及大小==

由上我们最新化简的形式：

$$
L(\omega)=||X\omega-y||^2=(X\omega-y)^T(X\omega-y)\\=(\omega^TX^T-y^T)(X\omega-y)\\=\omega^TX^TX\omega-\omega^TX^Ty-y^TX\omega+y^Ty\\=\omega^TX^TX\omega-2\omega^TX^Ty+y^Ty
$$

这里我们运用到了一些矩阵的性质：
* 矩阵乘法的分配率
* $y^TX\omega$的结果是一个实数，且由于一个实数的转置就是其本身，我们有$\omega^TX^Ty=(y^TX\omega)^T=y^TX\omega$
* 转置的运算性质

通过我们化简得到的最终形式，我们对损失函数$L(\omega)$进行求$\omega$的偏导，即：

$$
\frac{\partial{L(\omega)}}{\partial{\omega}}=\frac{\partial{(\omega^TX^TX\omega-2\omega^TX^Ty+y^Ty)}}{\partial{\omega}}\\=\frac{\partial{(\omega^TX^TX\omega)}}{\partial{\omega}}-2\frac{\partial{\omega^TX^Ty}}{\partial{\omega}}+0\\=2X^TX\omega-2X^Ty
$$

此处我们用到了一些矩阵求导的相关公式：
* $\frac{\partial{(X^TAX)}}{\partial{X}}=AX+A^TX$
此处我们将$X^TX$视作一个整体
* $\frac{\partial{(X^Ta)}}{\partial{X}}=\frac{\partial{(a^TX)}}{\partial{X}}=a，其中a为常数向量$
  
最终，我们考虑此损失函数的偏导数为零，我们就可以得到：
$\omega=(X^TX)^{-1}X^Ty$

也就是得到了我们最终要的权重向量结果

# 3.逐步回归
# 3.1什么是逐步回归
逐步回归是回归分析中一种筛选变量的过程，我们可以使用逐步回归从一组候选变量中构建回归模型，让系统自动识别出有影响的变量。

* 逐步回归法的基本思想是将变量逐个引入模型，每引入一个解释变量后都要进行F检验，并对已经选入的解释变量逐个进行t检验，当原来引入的解释变量由于后面解释变量的引入变得不再显著时，则将其删除。以确保每次引入新的变量之前回归方程中只包含显著性变量。
* 逐步回归法可以认为是向前引入法与向后剔除法的综合。逐步回归法克服了向前引入法与向后剔除法的缺点，吸收两种方法的优点。逐步回归法是以向前引入为主，变量可进可出的变量选取方法。它的基本思想是，当被选入的变量在新变量引入后变得不重要时，可以将其剔除，而被剔除的变量当它在新变量引入后变得重要时，又可以重新选入方程。
* 逐步回归分析是多元回归分析中的一种方法。回归分析是用于研究多个变量之间相互依赖的关系，而逐步回归分析往往用于建立最优或合适的回归模型，从而更加深入地研究变量之间的依赖关系。

依据上述思想，可利用逐步回归筛选并剔除引起多重共线性的变量，其具体步骤如下：先用结果变量对每一个所考虑的预测变量做简单回归，然后以对结果变量贡献最大的预测变量所对应的回归方程为基础，再逐步引入其余预测变量。经过逐步回归，使得最后保留在模型中的解释变量既是重要的，又没有严重多重共线性。

对逐步回归，我们有三种实现策略，一般为第三种：
1. 正向选择：从模型中没有预测因素开始，反复添加最有帮助的预测因素，直到没有显著的预测变量选入回归方程 。
2. 向后选择（向后消除）从完整模型（即包含所有可能预测变量的模型）中的所有预测变量开始，以迭代方式删除贡献最小的预测变量，直到没有不显著的预测变量从回归方程删除。
3. 逐步选择（顺序替换）这是向前和向后选择的组合。您从没有预测变量开始，然后顺序添加最有贡献的预测变量（例如正向选择）。添加每个新变量后，删除所有不再改善模型拟合的变量（例如向后选择），直到既没有显著的预测变量选入回归方程，也没有不显著的预测变量从回归方程中剔除为止。

注意：
* 正向选择和逐步选择可以应用在高维数据情况下，即样本数量n小于预测变量p的数量，例如在基因组领域，全基因组数量远大于测试人群。
* 向后选择要求样本数n大于变量p数，以便可以拟合整个模型。

# 4.从普通线性回归到广义线性回归

## 4.1普通的线性回归模型：
* 回归的最初含义——回归到均值，这就是回归模型的本质

往往用于三种场景：
* 描述自变量和因变量的因果关系，即数据产生的某种机制。
* 因变量的因果关系，即数据产生的某种机制。
* 得知自变量和因变量的关系后，应用新数据得到预测结果。

线性回归的模型往往需要满足数据的正态分布假设

## 4.2广义线性模型和狭义线性模型

广义线性模型旨在解决普通线性回归模型无法处理因变量离散，并发展能够解决非正态因变量的回归建模任务的建模方法。

在其框架下：**因变量不再要求连续、正态，当然自变量更加没有特殊的要求。能够对正态分布、二项分布、泊松分布、Gamma分布等随机因变量进行建模**

广义线性模型是普通线性模型的普遍化，如果把前述的普通线性回归模型称为狭义线性模型，那么它就是广义线性模型中因变量服从正态分布的特例

而事实上，对于正态分布、二项分布、泊松分布、Gamma分布等都有一个共同的特点：它们都可以写成一致的指数统计表达式，所以称为指数分布族，其统一的概率分布如下：

$$
p(y|\theta,\phi)=exp(\frac{y\theta-b(\theta)}{\phi}+c(y,\phi))
$$
