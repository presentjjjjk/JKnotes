# 回归分析
## 1.1是什么
回归分析是一种统计分析方式，用于预测一个连续变量与一个或是多个其他变量之间的关系。回归分析通过建立模型来预测因变量（被预测变量）与自变量（预测变量）之间的关系，从而预测被预测变量的值
## 1.2多种形式
线性回归、非线性回归、logistics回归等等
## 1.3几种的区别
线性回归：假设因变量与自变量之间存在线性关系，即因变量与自变量的关系可以用直线来描述，其通常用于预测数值型变量，如根据房屋面积、房龄等因素预测房价等
非线性回归：假设因变量和自变量之间不存在线性关系，即因变量与自变量的关系需要用更为复杂的函数模型来描述，例如预测某商家的销售额——我们通过分析广告费用、宣传渠道等预测销售额的变化趋势。
logistic回归：常常用于二元分类问题，例如预测某个用户是否会购买某种商品或预测某个患者是否患有某种疾病。该模型往往建立在Sigmoid函数的基础上。该函数可以将任意实数映射到0-1的范围内，表示某事件的概率
# 2.线性回归
## 2.1线性
线性包括了可加性和齐次性
### 2.1.1可加性
也称作是叠加性
即当函数$f$满足下式$$f(x+y)=f(x)+f(y)$$
则我们称函数f具有可加性
### 2.1.2齐次性
也称作是均匀性
即当函数$f$满足下式$$f(ax)=af(x)$$
其中a为与x无关的常数，则我们称函数$f$具有齐次性

当函数$f$同时具有可加性与齐次性时，即函数$f$满足 $$f(ax+by)=af(x)+bf(y)$$
我们称函数$f$为线性函数
## 2.2回归
解决回归问题，实际上就是找到一条线/超平面来拟合这些样本点，使他们之间的误差尽可能的小。我们所构建的不同的线/平面在同一个数据集下对应着不同的误差，我们需要找到使得误差最小的线/超平面，这就是回归的目的。
### 2.2.1损失函数
如何定量描述我们上面说到的这种误差来确定我们的拟合函数呢？
我们引入损失函数（误差函数）来衡量误差
常用的损失函数有：
均方误差MSE：$\frac{1}{m}\displaystyle\sum_{i=1}^m(y_i-\hat{y_i})^2$
均方根误差RMSE：$\sqrt{\frac{1}{m}\displaystyle\sum_{i=1}^m(y_i-\hat{y_i})^2}$
平均绝对误差MAE：$\frac{1}{m}\displaystyle\sum_{i=1}^m|y_i-\hat{y_i}|$
### 2.2.2回归任务的模型评估方式
R-squared:$$R^2(y,\hat{y})=1-\frac{\displaystyle\sum_{i=0}^n(y_i-\hat{y_i})^2}{\displaystyle\sum_{i=0}^n(y_i-\bar{y})^2}$$
此方式我们可以理解为按照模型预测，比较模型预测和按照平均值预测的比例，此比例越小，那么模型就越精确，即上式越接近1，模型拟合的越好；当该式子出现负数的情况时，说明该模型的预测不如全部按照平均值进行预测（式子值=0）
### 2.2.3最小化损失函数
我们再一次强调：**回归任务的目的是拟合样本点，使得误差尽可能的小**
### 2.2.4最小二乘法
#### 2.2.4.1一元线性函数
我们先以一元线性函数为例考虑线性回归，之后再引入多元线性回归
考虑一元线性函数$y=kx+b$，并采用均方误差作为我们的损失函数，则损失函数为关于变量k和b的函数：$$L(k,b)=\frac{1}{m}\displaystyle\sum_{i=1}^m((kx_i+b)-y_i)^2$$
其中，m为样本个数；而我们的回归任务就是找到这个$L(k,b)$函数的最小值。
我们通过求偏导数的方法来求这个函数的最小值：
对L函数求b的偏导（过程比较简单不再列出），我们可以求出当偏导为0时有：$$k\displaystyle\sum_{i=1}^mx_i+\displaystyle\sum_{i=1}^mb-\displaystyle\sum_{i=1}^my_i=0$$
即$km\bar{x}+mb-m\bar{y}=0$
即$$b=\bar{y}-k\bar{x}$$
(联系到均值公式)
同样的，我们对L函数求k的偏导，（过程简单省去），则当偏导为0时我们有：
$$k\displaystyle\sum_{i=1}^mx_i^2+b\displaystyle\sum_{i=1}^mx_i-\displaystyle\sum_{i=1}^mx_iy_i$$
考虑x均值的情况下此式改写为：$$k\displaystyle\sum_{i=1}^mx_i^2+mb\bar{x}-\displaystyle\sum_{i=1}^mx_iy_i=0$$
结合最小时$b=\bar{y}-k\bar{x}$,我们消去b可以得到：
$$k(\displaystyle\sum_{i=1}^mx_i^2-m\bar{x}^2)=\displaystyle\sum_{i=1}^mx_iy_i-m\bar{y}\bar{x}$$
从而我们可也解得$$k=\frac{\displaystyle\sum_{i=1}^mx_iy_i-m\bar{x}\bar{y}}{\displaystyle\sum_{i=1}^mx_i^2-m\bar{x}^2}$$
这样我们就得到了使得该损失函数最小的k，b参数。
#### 2.2.4.2多元线性函数
现在我们将一元变量推广到多元变量，考虑多元函数式为$$f(x_1,x_2,\ldots,x_n)=\omega_1x_1+\omega_2x_2+\ldots+\omega_nx_n+b$$
我们通过线性代数向量概念对该式子进行整理，不妨$\omega_0=b,x_0=1$，那么我们有权重向量$\omega$和特征向量$x$，且$$\omega=\begin{pmatrix}\omega_0,&\omega_1,&\omega_2,&\ldots,&\omega_n\end{pmatrix}$$
$$x=\begin{pmatrix}x_0,&x_1,&x_2,&\ldots,&x_n\end{pmatrix}$$
则我们的多元函数可以写成$f(x)=\omega^Tx或f(x)=x^T\omega$
从而我们的损失函数可以写成如下形式:$$L(\omega)=\displaystyle\sum_{i=1}^m(y_i-\omega^Tx^{(i)})^2$$
其中，$y_i$是第i个真实值,$x^{(i)}$第i个样本的特征向量
（PS：这里要注意，真实值，标签值和样本值这三个词的意思其实是一样的）
继续，我们希望对式子进行进一步的简化，定义如下：
* 标签向量：将m个样本标签值堆叠成一个标签向量y，$y=(y_1,y_2,\ldots,y_m),
==在线性代数当中，见到一个向量均默认为列向量，转置的才为行向量==
* 样本矩阵X：定义样本矩阵X，形状为(m,n+1)的矩阵，其中有m个样本，n+1个特征（其中第1个特征向量的值均为1）
$$\begin{pmatrix}1&x_1^{(1)}&x_2^{(1)}&\dots&x_n^{(1)}\\1&x_1^{(2)}&x_2^{(2)}&\dots&x_n^{(2)}\\\vdots&\vdots&\vdots&\ddots&\vdots\\1&x_1^{(m)}&x_2^{(m)}&\dots&x_n^{(m)}\end{pmatrix}$$
在该矩阵当中，每一行是一个样本，每一列是一个特征(向量)
从而我们可以得到：将样本矩阵与权重向量相乘得到预测值向量$\hat{y}$
即：$$\begin{pmatrix}1&x_1^{(1)}&x_2^{(1)}&\dots&x_n^{(1)}\\1&x_1^{(2)}&x_2^{(2)}&\dots&x_n^{(2)}\\\vdots&\vdots&\vdots&\ddots&\vdots\\1&x_1^{(m)}&x_2^{(m)}&\dots&x_n^{(m)}\end{pmatrix}\begin{pmatrix}
\omega_0\\\vdots\\\omega_n    
\end{pmatrix}=\begin{pmatrix}
\hat{y}_0\\\vdots\\\hat{y}_m    
\end{pmatrix}$$
##### 2.2.4.2.12范数
同时我们还需要借用2范数进行化简，下作介绍：
2范数：向量各个元素的平方和的平方根，即$||x||_2=\sqrt{\displaystyle\sum_{i=1}^nx_i^2}$
其中|| ||是范数符号，下标2表示为2范数，其有以下特性：
$x^Tx=||x||_2^2=\displaystyle\sum_{i=1}^nx_i^2$
又，此时我们的损失函数已经简化为如下形式：$$L(\omega)=||X\omega-y||^2$$
==注意矩阵和向量的形状及大小==
由上我们最新化简的形式：$$L(\omega)=||X\omega-y||^2=(X\omega-y)^T(X\omega-y)\\=(\omega^TX^T-y^T)(X\omega-y)\\=\omega^TX^TX\omega-\omega^TX^Ty-y^TX\omega+y^Ty\\=\omega^TX^TX\omega-2\omega^TX^Ty+y^Ty$$
这里我们运用到了一些矩阵的性质：
* 矩阵乘法的分配率
* $y^TX\omega$的结果是一个实数，且由于一个实数的转置就是其本身，我们有$\omega^TX^Ty=(y^TX\omega)^T=y^TX\omega$
* 转置的运算性质

通过我们化简得到的最终形式，我们对损失函数$L(\omega)$进行求$\omega$的偏导，即：
$\frac{\partial{L(\omega)}}{\partial{\omega}}=\frac{\partial{(\omega^TX^TX\omega-2\omega^TX^Ty+y^Ty)}}{\partial{\omega}}\\=\frac{\partial{(\omega^TX^TX\omega)}}{\partial{\omega}}-2\frac{\partial{\omega^TX^Ty}}{\partial{\omega}}+0\\=2X^TX\omega-2X^Ty$
此处我们用到了一些矩阵求导的相关公式：
* $\frac{\partial{(X^TAX)}}{\partial{X}}=AX+A^TX$
此处我们将$X^TX$视作一个整体
* $\frac{\partial{(X^Ta)}}{\partial{X}}=\frac{\partial{(a^TX)}}{\partial{X}}=a，其中a为常数向量$
最终，我们考虑此损失函数的偏导数为零，我们就可以得到：
$\omega=(X^TX)^{-1}X^Ty$
也就是得到了我们最终要的权重向量结果

# 3.逐步回归