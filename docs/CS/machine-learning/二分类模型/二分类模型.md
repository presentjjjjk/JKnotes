# 二分类模型
最常见的分类任务是二分类任务,即训练集的输出只有两个值---0和1,我们的任务是训练一个分类器,当给出新的输入的时候,能够快速将它划分到某个类别当中去.显然,这个分类器应该是一个函数,当输入满足某些条件,函数值为0,满足另一些条件,函数值为1,最自然能想到的分类函数就是阶跃函数:

!!!阶跃函数
    ![示例图片](image-1.png){ width=300px style="border-radius: 10px;" }

当x大于0的时候,分类为1,当x<0的时候分类为0,但是,这是一个分段的不连续的函数,不利于我们后续的操作,所以,要想办法找到一个和阶跃函数比较像的函数作为我们的分类器.

![示例图片](image-2.png){ width=300px style="border-radius: 10px;" }

我们采取的分类函数就是如图所示的sigmod函数,这个函数有着类似阶跃函数的形状,能够通过一个线性或者非线性映射将输入转化成$z$,再输出它被分类的一个概率值,它的函数表达式为:

$$
g(z)=\frac{1}{1+e^{-z}}
$$

下一步就是把输入映射到z的定义域上,注意到,输入有可能是多维的,最常见的映射方式就是线性变换:

记输入为向量$\vec{x}$,那么通常有:

$$
z=\vec{w}\cdot \vec{x}+b=w_1x_1+w_2x_2+...+w_nx_n+b
$$

当线性映射不满足要求的时候,也可以使用多项式映射,例如:

$$
z=w_1x_1+w_2x_2^2+w_3x_3^3
$$

来获得比较好的转换效果,这一点可以从决策边界上看出来.

于是我们就得到决策函数:

$$
f_{\vec{w},b}(\vec{x})=g(\vec{w} \cdot\vec{x}+b)=\frac{1}{1+e^{\vec{w} \cdot\vec{x}+b}}
$$

其中$z=\vec{w}\cdot \vec{x}+b$可以替换成多项式函数$z=f(\vec{x},\vec{w})$

## 决策边界
我们知道,当z大于0的时候,可以认为输出的都是1,当z小于0的时候,可以认为输出都是0,所以z=0也就被称作决策边界.

例如,一维的时候:

$$
z=wx+b
$$

决策边界就为:

$$
x=-\frac{b}{w}
$$

为数轴上的一个点

二维的时候,决策边界为:

$$
w_1x_1+w_2x_2+b=0
$$

是一条直线

![示例图片](image-3.png){ width=300px style="border-radius: 10px;" }

如图所示,决策边界将两者分成的鲜明的两大类.

从中我们也不难想到,当两种输出的点之间的界限没有这么清晰,或者很难用直线表示的时候,直线的决策边界也就是线性映射的效果肯定不会很好,这个时候,就需要对点集做非线性映射,得到的决策边界也是非线性的:

![示例图片](image-4.png){ width=300px style="border-radius: 10px;" }

例如这个图,圆形的决策边界可以很好的满足要求

## 代价函数

![示例图片](image-5.png){ width=400px style="border-radius: 10px;" }

仿照线性回归的思想,我们用均方误差来代表使用这样的分类函数$f_{\vec{w},b}(\vec{x})$的代价:

$$
J(\vec{w},b)=\frac{1}{2m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}_i)-y_i)^2
$$

但是,在求解最小值的时候会遇到困难,这家伙是非凸函数,存在许许多多个极小值点,这样,我们在使用梯度下降算法的时候会遇到困难,容易收敛到局部最小值也就是极小值,所以,人们决定不采用这样的代价函数,转而采用了另一种凸的满足条件的代价函数

!!!代价函数非凸
    ![示例图片](image-6.png){ width=300px style="border-radius: 10px;" }


定义损失函数$L(f_{\vec{w},b}(\vec{x}_i),{y}_i)$

$$
L(f_{\vec{w},b}(\vec{x}_i),y_i)=\begin{cases}
    -log(f_{\vec{w},b}(\vec{x_i})) \quad y_i=1\\
    -log(1-f_{\vec{w},b}(\vec{x_i}))\quad y_i=0
\end{cases}
$$

也可以直接写成:

$$
L(f_{\vec{w},b}(\vec{x}_i),y_i)=-y_ilog(f_{\vec{w},b}(\vec{x_i}))-(1-y_i)log(1-f_{\vec{w},b}(\vec{x_i}))
$$

代价函数为:

$$
J(w,b)=\frac{1}{m}\sum_{i=1}^mL(f_{\vec{w},b}(\vec{x}_i),y_i)
$$

这个函数具有较为良好的凹凸性,可以使用梯度下降算法直接求解它的最小值,进而得到最优的分类函数

!!!改进的代价函数
    ![示例图片](image-7.png){ width=400px style="border-radius: 10px;" }

## 代码示例

梯度下降公式为:

$$
w_j=w_j-\alpha \frac{\partial J}{\partial w_j}
$$

其中:

$$
\frac{\partial J}{\partial w_j}=\frac{1}{m}\sum_{i=1}^m \frac{\partial L_i}{\partial w_j}
$$

$$
\frac{\partial L_i}{\partial w_j}=(\frac{-y_i}{f_{\vec{w},b}(\vec{x}_i)}+\frac{1-y_i}{1-f_{\vec{w},b}(\vec{x}_i)})\frac{\partial f(\vec{x_i})}{\partial w_j}
$$

$$
\frac{\partial f(\vec{x_i})}{\partial w_j}=\frac{\partial g(z)}{\partial z}x_{ij}=\frac{e^{-z}}{(1+e^{-z})^2}x_{ij}
$$

综上会有:

$$
\frac{\partial J}{\partial w_j}=\frac{1}{m}\sum_{i=1}^m [-y_i(1-f(\vec{x_i}))+(1-y_i)f(\vec{x_i})]x_{ij}
$$

同理也会有:

$$
\frac{\partial J}{\partial b}=\frac{1}{m}\sum_{i=1}^m [-y_i(1-f(\vec{x_i}))+(1-y_i)f(\vec{x_i})]
$$

实际上,根据$y_i$的特殊性:

$$
[-y_i(1-f(\vec{x_i}))+(1-y_i)f(\vec{x_i})]=[f(x_i)-y_i]
$$

训练数据data.csv:

```
Feature1,Feature2,Label
2.7810836,2.550537003,0
1.465489372,2.362125076,0
3.396561688,4.400293529,0
1.38807019,1.850220317,0
3.06407232,3.005305973,0
7.627531214,2.759262235,1
5.332441248,2.088626775,1
6.922596716,1.77106367,1
8.675418651,-0.242068655,1
7.673756466,3.508563011,1

```

代码

```python
import numpy as np

import pandas as pd

from math import sqrt

import matplotlib.pyplot as plt

# 加载数据
data = pd.read_csv('data.csv')

# 分离特征和标签
X = data[['Feature1', 'Feature2']].values.tolist()  # 将特征转换为列表
y = data['Label'].tolist()  # 将标签转换为列表

#m个训练数据,分别是x=[X[i][0],X[i][1]]

#定义函数:

f=lambda x,w,b:1/(1+np.exp(-(w[0]*x[0]+w[1]*x[1]+b)))

#学习率

a=0.1

#终止条件

e=1e-5

# 系数
w=[1,1]
b=0


while True:

    # 存储上一次的w和b

    w_0=w[:]
    b_0=b

    for j in range(2):
        s=0
        t=0
        for i in range(len(y)):
            x=X[i]
            pL_ipw=(-y[i]*(1-f(w,x,b))+(1-y[i])*f(w,x,b))*x[j]
            s=s+pL_ipw
            t=t+(-y[i]*(1-f(w,x,b))+(1-y[i])*f(w,x,b))
        w[j]=w[j]-a*s/len(y)

    b=b-a*t/len(y)

    if abs(b-b_0)<e and sqrt((w[0]-w_0[0])**2+(w[1]-w_0[1])**2)<e:
        break

w=[float(i) for i in w]
b=float(b)

for i in range(len(w)):
    print(f'系数w{i:}:{w[i]:.3f}')

print(f'系数b:{b:.3f}')

z=[]

for i in range(len(y)):
    
    x=np.array(X[i])
    w=np.array(w)
    z.append(np.dot(w,x)+b)

plt.figure()

plt.scatter(z,y,color='r')

z_0 = np.linspace(min(z)-5, max(z)+5, 40)

y_0=1/(1+np.exp(-z_0))

plt.plot(z_0,y_0,color='g')

plt.show


```

!!! info "运行结果"

    ```
    系数w0:4.807
    系数w1:-6.998
    系数b:-3.019
    ```

    ![alt text](image-8.png)





